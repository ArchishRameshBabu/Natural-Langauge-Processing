{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANLP Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importing all necessary library \n",
    "import math\n",
    "import codecs\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 : Entropy of a 8 sided die\n",
    "\n",
    "$Entropy = -\\sum_{i=1}^{n}plogp$\n",
    "\n",
    "$p_1=(1/8) , p_2=(1/16) , p_3=(1/4) , p_4=(1/8) , p_5 = (1/16) , p_6=(1/4) , p_7=(1/4) , p_8= (1/16)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Entropy of the 8 sided die is 2.75\n"
     ]
    }
   ],
   "source": [
    "entropy = -1*( (1/8)*math.log2(1/8)   + (1/16)*math.log2(1/16)   + (1/4)*math.log2(1/4)   + (1/8)*math.log2(1/8)   \n",
    "              + (1/16)*math.log2(1/16)   + (1/16)*math.log2(1/16)   +(1/4)*math.log2(1/4)   + (1/16)*math.log2(1/16))\n",
    "\n",
    "print('The Entropy of the 8 sided die is',entropy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 part a\n",
    "\n",
    "Method <br>\n",
    "1.Read the text <br>\n",
    "2.Convert into lower case <br>\n",
    "3.Remove non essential characters <br>\n",
    "4.Calculate the frequency distribution <br>\n",
    "5.Caculate the total number of characters <br>\n",
    "6.Calculate the probablity of each letter <br>\n",
    "7.Measure the Entropy based on the probablity <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reading the text\n",
    "text=\"Bach was the most famous composer in the world, and so was Handel. Handel was half German, half Italian and half English. He was very large. Bach died from 1750 to the present. Beethoven wrote music even though he was deaf. He was so deaf he wrote loud music. He took long walks in the forest even when everyone was calling for him. Beethoven expired in 1827 and later died for this.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_case_text=text.lower() ## Coverting to lowercase \n",
    "Total_char=range(len(lower_case_text)) ## Creating a list using the length of text\n",
    "format_text=[] ## Empty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Removing all blank spaces,special char & numbers\n",
    "for i in Total_char:\n",
    "    if(lower_case_text[i] not in (' ','0','1','2','3','4','5','6','7','8','9','.',',')):format_text.append(lower_case_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating the frequency of each character in the text\n",
    "Freq_dist= Counter(format_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating the total lenght of the array\n",
    "totalSpam = sum(Freq_dist.values())\n",
    "####print(totalSpam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unique letters in the text \n",
    "unique_letters=list(set(format_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "char=[]\n",
    "prob=[]\n",
    "entropy=[]\n",
    "\n",
    "for i in unique_letters:\n",
    "    char.append(i)\n",
    "    val=Freq_dist[i]/totalSpam\n",
    "    prob.append(val)\n",
    "    x=0\n",
    "    if(val>0):x=-1*(math.log2(val))*val\n",
    "    entropy.append(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Alphabet  Probablity   Entropy\n",
      "0         d    0.047619  0.209158\n",
      "1         c    0.020408  0.114586\n",
      "2         m    0.027211  0.141488\n",
      "3         l    0.051020  0.219019\n",
      "4         i    0.047619  0.209158\n",
      "5         t    0.057823  0.237781\n",
      "6         h    0.081633  0.295078\n",
      "7         p    0.010204  0.067497\n",
      "8         s    0.064626  0.255385\n",
      "9         b    0.013605  0.084349\n",
      "10        w    0.040816  0.188356\n",
      "11        g    0.020408  0.114586\n",
      "12        r    0.051020  0.219019\n",
      "13        v    0.020408  0.114586\n",
      "14        y    0.006803  0.048977\n",
      "15        n    0.068027  0.263792\n",
      "16        u    0.017007  0.099962\n",
      "17        a    0.091837  0.316358\n",
      "18        f    0.034014  0.165910\n",
      "19        k    0.006803  0.048977\n",
      "20        e    0.142857  0.401051\n",
      "21        o    0.074830  0.279882\n",
      "22        x    0.003401  0.027890\n"
     ]
    }
   ],
   "source": [
    "data_frame = pd.DataFrame({'Alphabet': char,'Probablity': prob,'Entropy': entropy})\n",
    "print(data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 part c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00962379591911875\n"
     ]
    }
   ],
   "source": [
    "##Variance of Entropy\n",
    "print(data_frame[\"Entropy\"].var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 part b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need atleast 5 digits since 4 digits can only provide 16 distinct combinations and we have 26 alphabets in the english laungauge off which 23 occur in the above text \n",
    "\n",
    "Bit Encoding for each letter   \n",
    "a=00000 , \n",
    "b=00001 , \n",
    "c=00010 , \n",
    "d=00011 , \n",
    "e=00100 , \n",
    "f=00101 , \n",
    "g=00110 , \n",
    "h=00111 , \n",
    "i=01000 , \n",
    "j=01001 , \n",
    "k=01010 , \n",
    "l=01011 , \n",
    "m=01100 , \n",
    "n=01101 , \n",
    "o=01110 , \n",
    "p=01111 , \n",
    "q=10000 , \n",
    "r=10001 , \n",
    "s=10010 , \n",
    "t=10011 , \n",
    "u=10100 , \n",
    "v=10101 , \n",
    "w=10110 , \n",
    "x=10111 , \n",
    "y=11000 , \n",
    "z=11001 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (Using Sub corpora A)\n",
    "\n",
    "1) Download the data and unzip it <br>\n",
    "2)Read the unzipped files which make up sub corpora A <br>\n",
    "3)Calculate the fequency for each char and use that information to calulate its corresponsing probablity <br>\n",
    "4)Use the probablity to calculate the entropy <br>\n",
    "5)Calculate the entropy <br>\n",
    "6)Display the net entropy and entropy of each character <br>\n",
    "\n",
    "PS: Kindly delete the folder 'TempData_ANLP_Assignment_ARAMESHB' after evaluating the assignment, the folder contains the SUSANNE corpa which was downloaded as a part of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Downloading the file from the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from shutil import unpack_archive\n",
    "from pathlib import Path\n",
    "\n",
    "#### The below portion of the code was taken from https://stackoverflow.com/questions/13137817/how-to-download-image-using-requests\n",
    "url = \"https://www.grsampson.net/SUSANNE.tgz\"\n",
    "filename = url.split(\"/\")[-1]\n",
    "with open(filename, \"wb\") as f:\n",
    "    r = requests.get(url)\n",
    "    f.write(r.content)\n",
    "\n",
    "my_file = Path(\"TempData_ANLP_Assignment_ARAMESHB/fc2/A01\")\n",
    "if my_file.is_file()!=True:\n",
    "    unpack_archive('SUSANNE.tgz','TempData_ANLP_Assignment_ARAMESHB')\n",
    "\n",
    "#### End of code portion taken from https://stackoverflow.com/questions/13137817/how-to-download-image-using-requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##text=open(\"D:/Education/Sem 1 IU My Stuff/ANLP/Data/fc2/A01\")\n",
    "text01= pd.read_csv(\"TempData_ANLP_Assignment_ARAMESHB/fc2/A01\",sep='\\t',header=None,names=[\"Row\",\"Ind\",\"PoS\",\"word\",\"word_modified\",\"other\"])\n",
    "text02= pd.read_csv(\"TempData_ANLP_Assignment_ARAMESHB/fc2/A02\",sep='\\t',header=None,names=[\"Row\",\"Ind\",\"PoS\",\"word\",\"word_modified\",\"other\"])\n",
    "text03= pd.read_csv(\"TempData_ANLP_Assignment_ARAMESHB/fc2/A03\",sep='\\t',header=None,names=[\"Row\",\"Ind\",\"PoS\",\"word\",\"word_modified\",\"other\"])\n",
    "text04= pd.read_csv(\"TempData_ANLP_Assignment_ARAMESHB/fc2/A04\",sep='\\t',header=None,names=[\"Row\",\"Ind\",\"PoS\",\"word\",\"word_modified\",\"other\"])\n",
    "text05= pd.read_csv(\"TempData_ANLP_Assignment_ARAMESHB/fc2/A05\",sep='\\t',header=None,names=[\"Row\",\"Ind\",\"PoS\",\"word\",\"word_modified\",\"other\"])\n",
    "text06= pd.read_csv(\"TempData_ANLP_Assignment_ARAMESHB/fc2/A06\",sep='\\t',header=None,names=[\"Row\",\"Ind\",\"PoS\",\"word\",\"word_modified\",\"other\"])\n",
    "text07= pd.read_csv(\"TempData_ANLP_Assignment_ARAMESHB/fc2/A07\",sep='\\t',header=None,names=[\"Row\",\"Ind\",\"PoS\",\"word\",\"word_modified\",\"other\"])\n",
    "text08= pd.read_csv(\"TempData_ANLP_Assignment_ARAMESHB/fc2/A08\",sep='\\t',header=None,names=[\"Row\",\"Ind\",\"PoS\",\"word\",\"word_modified\",\"other\"])\n",
    "text09= pd.read_csv(\"TempData_ANLP_Assignment_ARAMESHB/fc2/A09\",sep='\\t',header=None,names=[\"Row\",\"Ind\",\"PoS\",\"word\",\"word_modified\",\"other\"])\n",
    "text10= pd.read_csv(\"TempData_ANLP_Assignment_ARAMESHB/fc2/A10\",sep='\\t',header=None,names=[\"Row\",\"Ind\",\"PoS\",\"word\",\"word_modified\",\"other\"])\n",
    "text11= pd.read_csv(\"TempData_ANLP_Assignment_ARAMESHB/fc2/A11\",sep='\\t',header=None,names=[\"Row\",\"Ind\",\"PoS\",\"word\",\"word_modified\",\"other\"])\n",
    "text12= pd.read_csv(\"TempData_ANLP_Assignment_ARAMESHB/fc2/A12\",sep='\\t',header=None,names=[\"Row\",\"Ind\",\"PoS\",\"word\",\"word_modified\",\"other\"])\n",
    "text13= pd.read_csv(\"TempData_ANLP_Assignment_ARAMESHB/fc2/A13\",sep='\\t',header=None,names=[\"Row\",\"Ind\",\"PoS\",\"word\",\"word_modified\",\"other\"])\n",
    "text14= pd.read_csv(\"TempData_ANLP_Assignment_ARAMESHB/fc2/A14\",sep='\\t',header=None,names=[\"Row\",\"Ind\",\"PoS\",\"word\",\"word_modified\",\"other\"])\n",
    "text19= pd.read_csv(\"TempData_ANLP_Assignment_ARAMESHB/fc2/A19\",sep='\\t',header=None,names=[\"Row\",\"Ind\",\"PoS\",\"word\",\"word_modified\",\"other\"])\n",
    "text20= pd.read_csv(\"TempData_ANLP_Assignment_ARAMESHB/fc2/A20\",sep='\\t',header=None,names=[\"Row\",\"Ind\",\"PoS\",\"word\",\"word_modified\",\"other\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Append the data together\n",
    "text=text01.append([text02,text03,text04,text05,text06,text07,text08,text09,text10,text11,text12,text13,text14,text19,text20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39285, 6)\n"
     ]
    }
   ],
   "source": [
    "###Total length of appended corpus\n",
    "print(text.shape) ###(39285, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Frequency of each POS\n",
    "freq=text.groupby(\"PoS\").agg('count')\n",
    "freq2=freq[\"word\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Total count \n",
    "total_count=text.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##List with all unique tags \n",
    "unique_tag=text['PoS'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Probality calculation\n",
    "prob=freq2/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net Entropy from all POS TAGS = 6.269610053587208\n",
      "    POS TAG   Entropy\n",
      "0        YB  0.115144\n",
      "1        AT  0.250305\n",
      "2      NP1s  0.152452\n",
      "3    NNL1cb  0.015088\n",
      "4        JJ  0.195967\n",
      "5      NN1c  0.227502\n",
      "6      VVDv  0.119243\n",
      "7      NPD1  0.025696\n",
      "8       AT1  0.125623\n",
      "9      NN1n  0.199483\n",
      "10       IO  0.130256\n",
      "11     NP1t  0.036563\n",
      "12       GG  0.049234\n",
      "13      YIL  0.055405\n",
      "14      ATn  0.010979\n",
      "15     NN1u  0.065396\n",
      "16      YIR  0.053564\n",
      "17      CST  0.047024\n",
      "18      DDy  0.006049\n",
      "19      NN2  0.198315\n",
      "20    NNL1c  0.056808\n",
      "21       YF  0.174246\n",
      "22      RRR  0.008359\n",
      "23       II  0.186292\n",
      "24    NNT1c  0.055827\n",
      "25       YH  0.055405\n",
      "26       JB  0.018205\n",
      "27    NNJ1c  0.052421\n",
      "28       YC  0.192725\n",
      "29     DDQr  0.019905\n",
      "..      ...       ...\n",
      "312    CS33  0.000388\n",
      "313     MCo  0.001044\n",
      "314     ZZ1  0.000388\n",
      "315  PPHO1f  0.001044\n",
      "316     MCe  0.002219\n",
      "317     MFn  0.000388\n",
      "318    RG21  0.000388\n",
      "319    RG22  0.000388\n",
      "320   DDo21  0.001044\n",
      "321   DDo22  0.001044\n",
      "322   CSk21  0.000388\n",
      "323   CSk22  0.000388\n",
      "324     CSg  0.000388\n",
      "325    NNux  0.000388\n",
      "326    IW21  0.000388\n",
      "327    IW22  0.000388\n",
      "328   RAc21  0.000388\n",
      "329   RAc22  0.000388\n",
      "330     LEn  0.000388\n",
      "331     CCn  0.000726\n",
      "332  NP1j31  0.000388\n",
      "333  NP1j32  0.000388\n",
      "334  NP1j33  0.000388\n",
      "335    RL21  0.000388\n",
      "336    RL22  0.000388\n",
      "337    PN1z  0.000388\n",
      "338    NP2s  0.001936\n",
      "339   REX21  0.000388\n",
      "340   REX22  0.000388\n",
      "341     RLw  0.000388\n",
      "\n",
      "[342 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "##Entropy calculation \n",
    "Entropy=[]\n",
    "POS_tag=[]\n",
    "for letter in unique_tag: \n",
    "    p_value=prob[letter]\n",
    "    p_value_log=math.log2(p_value)\n",
    "    Entropy.append(-1*p_value*p_value_log) \n",
    "    POS_tag.append(letter)\n",
    "\n",
    "    \n",
    "Solution_3 = pd.DataFrame({'POS TAG': POS_tag,'Entropy': Entropy})\n",
    "Net_Entropy=Solution_3['Entropy'].sum()\n",
    "print('Net Entropy from all POS TAGS =',Net_Entropy)\n",
    "\n",
    "print(Solution_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
