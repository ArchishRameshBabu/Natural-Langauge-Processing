{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANLP Assignment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.) Assuming you do not know the code beforehand -- i.e., your task is to decrypt a message -- describe what the HMM would look like, in order to decode such words. How many and which states will the HMM have, and what do the emissions look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most popular applications of HMMs models is cryptoanalysis. There are 26 unique characters in the English langauge without considering special characters, for the below explanation we will only consider the 26 alphabets only. \n",
    "\n",
    "We will also consider 26 hidden layers, one for each character. It is possible to consider even less hidden states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform HMM we would need an Initial state, transition state, Initial Emission state \n",
    "\n",
    "Initial State : We can calculate the frequency distribution of alphabets for both the encrypted data and some English corpus. We can then match the alphabet's with the highest frequency in both together, similarly the 2nd highest in both and so on for all 26 characters\n",
    "Size of Initial Matrix : 1x26\n",
    "\n",
    "Transition Matrix : We can construct a transition matrix from the frequency distribution of the Encrypted text \n",
    "Size : 26 x 26 \n",
    "\n",
    "Emission Matrix : We can assume some random values for this and we can train our HMM model to get the final Emission Matrix\n",
    "Size : 26 x 26 \n",
    "\n",
    "Algorithm for Updating Emission State: \n",
    "1) Compute the Forward probability and then the backward probability \n",
    "2) Use the Baumâ€“Welch algorithm to recalculate Emission Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.) Make a suggestion for how the emission probabilities should be distributed for a well-trained HMM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a well trained HMM model the probability of each Encrypted character should have only one Hidden state with a very high Emission probability as compared to other Hidden states. Below is an example where there are 3 encrypted characters and 3 corresponding alphabets, from the matrix it could be seen that Enc 1=c, Enc 2=a, Enc 3=b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   | Enc 1 | Enc 2 | Enc 3 |\n",
    "|---|-------|-------|-------|\n",
    "| a | 0.01  | 0.99  | 0.01  |\n",
    "| b | 0.05  | 0     | 0.89  |\n",
    "| c | 0.94  | 0.01  | 0.10  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.) Would it make sense to convert the HMM to a trigram model instead of a bigram model? What are the advantages, what are the disadvantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice will primarily depend upon the distribution of the langauge, the ngram model that gives the best distribution between the character sets should be considered provided that it doesnot become computationally too expensive. \n",
    "\n",
    "Adv : </br>\n",
    "1) Since there will more states in a trigram model, the overall accuracy will be higher while decrypting complex text\n",
    "    \n",
    "Dis : \n",
    "1) It will be computationally more expensive 2)Bigram would work better for simple encryption\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wikivisually.com/wiki/Baum%E2%80%93Welch_algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating the input data\n",
    "Initial_Matrix = np.matrix(\"0.45 0.35 0.15 0.05\")\n",
    "Transition_matrix = np.matrix(\"0.03 0.42 0.50 0.05; 0.01 0.25 0.65 0.09; 0.07 0.03 0.15 0.75; 0.30 0.25 0.15 0.30\")\n",
    "EmissionMatrix = np.matrix(\"0.84 0.05 0.03 0.05; 0.01 0.10 0.45 0.10; 0.02 0.02 0.02 0.60; 0.01 0.70 0.25 0.05; 0.12 0.13 0.25 0.20\")\n",
    "Final_Matrix=np.zeros(24)\n",
    "Final_Matrix=np.reshape(Final_Matrix,(4,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculating the first coulmn for the Alpha Matrix\n",
    "Final_Matrix.T[0]=np.multiply(Initial_Matrix,EmissionMatrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Second columns calculation for the Alpha Matrix\n",
    "s=np.zeros(4)\n",
    "for i in range(4):\n",
    "    x=(np.multiply(Final_Matrix.T[0],Transition_matrix.T[i]))\n",
    "    s[i]=x.sum()\n",
    "\n",
    "final=np.multiply(s,EmissionMatrix[1])\n",
    "Final_Matrix.T[1]=final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Third columns calculation for the Alpha Matrix\n",
    "s=np.zeros(4)\n",
    "for i in range(4):\n",
    "    x=(np.multiply(Final_Matrix.T[1],Transition_matrix.T[i]))\n",
    "    s[i]=x.sum()\n",
    "\n",
    "final=np.multiply(s,EmissionMatrix[2])\n",
    "Final_Matrix.T[2]=final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Fourth columns calculation for the Alpha Matrix\n",
    "s=np.zeros(4)\n",
    "for i in range(4):\n",
    "    x=(np.multiply(Final_Matrix.T[2],Transition_matrix.T[i]))\n",
    "    s[i]=x.sum()\n",
    "\n",
    "final=np.multiply(s,EmissionMatrix[0])\n",
    "Final_Matrix.T[3]=final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Fifth columns calculation for the Alpha Matrix\n",
    "s=np.zeros(4)\n",
    "for i in range(4):\n",
    "    x=(np.multiply(Final_Matrix.T[3],Transition_matrix.T[i]))\n",
    "    s[i]=x.sum()\n",
    "\n",
    "final=np.multiply(s,EmissionMatrix[3])\n",
    "Final_Matrix.T[4]=final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Sixth columns calculation for the Alpha Matrix\n",
    "s=np.zeros(4)\n",
    "for i in range(4):\n",
    "    x=(np.multiply(Final_Matrix.T[4],Transition_matrix.T[i]))\n",
    "    s[i]=x.sum()\n",
    "\n",
    "final=np.multiply(s,EmissionMatrix[4])\n",
    "Final_Matrix.T[5]=final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Results from the Forward Propogation is : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.78000000e-01 1.25800000e-04 1.45011130e-04 1.06482195e-02\n",
      "  5.33952993e-06 1.78656581e-05]\n",
      " [1.75000000e-02 1.63895000e-02 1.49688970e-04 5.32158902e-04\n",
      "  3.34182452e-03 1.16053595e-04]\n",
      " [4.50000000e-03 9.06412500e-02 4.93625250e-04 1.96856178e-04\n",
      "  1.44931098e-03 5.99794373e-04]\n",
      " [2.50000000e-03 2.46000000e-03 4.21201695e-02 6.51349618e-04\n",
      "  4.61676149e-05 2.80372941e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(Final_Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha_4_NN 0.00019685617823999994\n",
      "Alpha_3_VB 0.04212016949999999\n",
      "Alpha_1_DT 0.378\n"
     ]
    }
   ],
   "source": [
    "print(\"Alpha_4_NN\",Final_Matrix[2,3])\n",
    "print(\"Alpha_3_VB\",Final_Matrix[3,2])\n",
    "print(\"Alpha_1_DT\",Final_Matrix[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backword Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Matrix_2=np.zeros(24)\n",
    "Final_Matrix_2=np.reshape(Final_Matrix_2,(4,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calulating the final Column for the Beta Matrix\n",
    "Final_Matrix_2.T[5]=np.multiply([1,1,1,1],EmissionMatrix[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Sixth columns calculation\n",
    "s=np.zeros(4)\n",
    "for i in range(4):\n",
    "    x=(np.multiply(Final_Matrix_2.T[5],Transition_matrix[i]))\n",
    "    s[i]=x.sum()\n",
    "\n",
    "final=np.multiply(s,EmissionMatrix[3])\n",
    "Final_Matrix_2.T[4]=final\n",
    "\n",
    "s=np.zeros(4)\n",
    "for i in range(4):\n",
    "    x=(np.multiply(Final_Matrix_2.T[4],Transition_matrix[i]))\n",
    "    s[i]=x.sum()\n",
    "\n",
    "final=np.multiply(s,EmissionMatrix[0])\n",
    "Final_Matrix_2.T[3]=final\n",
    "\n",
    "s=np.zeros(4)\n",
    "for i in range(4):\n",
    "    x=(np.multiply(Final_Matrix_2.T[3],Transition_matrix[i]))\n",
    "    s[i]=x.sum()\n",
    "\n",
    "final=np.multiply(s,EmissionMatrix[2])\n",
    "Final_Matrix_2.T[2]=final\n",
    "\n",
    "s=np.zeros(4)\n",
    "for i in range(4):\n",
    "    x=(np.multiply(Final_Matrix_2.T[2],Transition_matrix[i]))\n",
    "    s[i]=x.sum()\n",
    "\n",
    "final=np.multiply(s,EmissionMatrix[1])\n",
    "Final_Matrix_2.T[1]=final\n",
    "\n",
    "s=np.zeros(4)\n",
    "for i in range(4):\n",
    "    x=(np.multiply(Final_Matrix_2.T[1],Transition_matrix[i]))\n",
    "    s[i]=x.sum()\n",
    "\n",
    "final=np.multiply(s,EmissionMatrix[0])\n",
    "Final_Matrix_2.T[0]=final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Beta Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.11199151e-03 8.11942904e-06 8.21746124e-05 7.42751184e-02\n",
      "  1.93200000e-03 1.20000000e-01]\n",
      " [1.61891116e-04 1.39966112e-04 4.40158343e-05 3.53594100e-03\n",
      "  1.49940000e-01 1.30000000e-01]\n",
      " [3.18800348e-05 4.86677460e-03 1.43793640e-04 5.50528200e-04\n",
      "  4.99500000e-02 2.50000000e-01]\n",
      " [4.49298632e-05 4.37179056e-04 1.43818839e-02 2.40235500e-03\n",
      "  8.30000000e-03 2.00000000e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(Final_Matrix_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta_4_NN 0.0005505282000000001\n",
      "Beta_2_NN 0.00486677460100194\n"
     ]
    }
   ],
   "source": [
    "print(\"Beta_4_NN\",Final_Matrix_2[2,3])\n",
    "print(\"Beta_2_NN\",Final_Matrix_2[2,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
